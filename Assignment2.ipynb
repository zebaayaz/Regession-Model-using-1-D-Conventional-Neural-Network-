{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXL7i+yHNfNEisicvhfkRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zebaayaz/Regession-Model-using-1-D-Conventional-Neural-Network-/blob/master/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBIqOgCBb6ue",
        "colab_type": "code",
        "outputId": "bcbac912-2932-4d91-e7d5-243d565fb43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "#To split the data into training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#To manipulate data\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiCANAjX0MOd",
        "colab_type": "code",
        "outputId": "776fb1c9-2f2d-4ea4-ab6f-fd6d8a8defd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"gdrive/My Drive/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJSFmk-3cXoi",
        "colab_type": "code",
        "outputId": "7ee557ca-4ac8-4ad3-cc33-e1196ade8925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "\n",
        "# import data from the link\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\\t')\n",
        "data.head(10)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "5         6  ...          2\n",
              "6         7  ...          2\n",
              "7         8  ...          2\n",
              "8         9  ...          2\n",
              "9        10  ...          2\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjtBVbMvOoRE",
        "colab_type": "code",
        "outputId": "ea610437-eba9-45fa-87e5-8d605ebd2110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check the shape of df\n",
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156060, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7St9lrUNylUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "579352c2-256e-4eab-b0b8-d27cfcd2dae4"
      },
      "source": [
        "#libraries used for data preprocessing\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b326RHXkyrbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "278f4c72-2f80-4032-f461-d83a890eb5e5"
      },
      "source": [
        "#splitting each review into documents\n",
        "documents = []\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "  tmpWords = word_tokenize(data['Phrase'][i])\n",
        "  documents.append((tmpWords, data['Sentiment'][i]))\n",
        "\n",
        "print(documents[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['A', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose'], 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC1lLgvXy_Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2eb239c-3745-4a92-92b5-27ce9a725b8a"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations=\"?:!.,;'\\\"-()\"\n",
        "\n",
        "#parameters to adjust to see the impact on outcome\n",
        "remove_stopwords = True\n",
        "useStemming = True\n",
        "useLemma = False\n",
        "removePuncs = True\n",
        "\n",
        "for l in range(len(documents)):\n",
        "  label = documents[l][1]\n",
        "  tmpReview = []\n",
        "  for w in documents[l][0]:\n",
        "    newWord = w\n",
        "    if remove_stopwords and (w in stopwords_en):\n",
        "      continue\n",
        "    if removePuncs and (w in punctuations):\n",
        "      continue\n",
        "    if useStemming:\n",
        "      \n",
        "      newWord = lancaster.stem(newWord)\n",
        "    if useLemma:\n",
        "      newWord = wordnet_lemmatizer.lemmatize(newWord)\n",
        "    tmpReview.append(newWord)\n",
        "  documents[l] = (' '.join(tmpReview), label)\n",
        "print(documents[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a sery escapad demonst ad good goos', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVME-tduzGud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = pd.DataFrame(documents, columns=['Phrase', 'Sentiment'])\n",
        "x_train, x_test, y_train, y_test = train_test_split(all_data['Phrase'], all_data['Sentiment'], train_size = 0.7, shuffle = True, random_state = 2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeZ-OFZgzMif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vectorize the data \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words = 'english', ngram_range= (1,1), max_features=1500)\n",
        "X = vectorizer.fit_transform(all_data['Phrase'])\n",
        "x_train = vectorizer.transform(x_train)\n",
        "x_test = vectorizer.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVn3gwGDzPbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_np = x_train.toarray()\n",
        "y_train_np = to_categorical(y_train)\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9YftJLozTdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZgujDRWzW6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "#model creation\n",
        "model = Sequential()\n",
        "#defing the layers\n",
        "model.add(Conv1D(filters = 128, kernel_size=1, activation='relu', input_shape=(x_train_np.shape[1],1)))\n",
        "model.add(Conv1D(filters = 128, kernel_size=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size =2))\n",
        "model.add(Flatten())\n",
        "keras.layers.Dropout(0.1, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmClIxiHze2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "aec6f53e-693e-4aae-d5bd-ef4fe8b6be07"
      },
      "source": [
        "#model compilation and optimizer used is Stochastic Gradient Descent\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ3O11dczsYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e23bfd1-d6e0-4dd7-84c3-3bb171208987"
      },
      "source": [
        "#Trainig the model\n",
        "model.fit(x_train, y_train_np, validation_data=(x_test, y_test_np), epochs = 50, batch_size = 128)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 109242 samples, validate on 46818 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "109242/109242 [==============================] - 31s 288us/step - loss: 1.2926 - acc: 0.5082 - val_loss: 1.2803 - val_acc: 0.5123\n",
            "Epoch 2/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.2797 - acc: 0.5089 - val_loss: 1.2733 - val_acc: 0.5123\n",
            "Epoch 3/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.2717 - acc: 0.5089 - val_loss: 1.2623 - val_acc: 0.5125\n",
            "Epoch 4/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.2579 - acc: 0.5097 - val_loss: 1.2456 - val_acc: 0.5151\n",
            "Epoch 5/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.2357 - acc: 0.5143 - val_loss: 1.2209 - val_acc: 0.5250\n",
            "Epoch 6/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.2075 - acc: 0.5231 - val_loss: 1.1912 - val_acc: 0.5289\n",
            "Epoch 7/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.1778 - acc: 0.5338 - val_loss: 1.1639 - val_acc: 0.5381\n",
            "Epoch 8/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.1506 - acc: 0.5450 - val_loss: 1.1402 - val_acc: 0.5540\n",
            "Epoch 9/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.1281 - acc: 0.5537 - val_loss: 1.1230 - val_acc: 0.5574\n",
            "Epoch 10/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.1120 - acc: 0.5611 - val_loss: 1.1124 - val_acc: 0.5610\n",
            "Epoch 11/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.1007 - acc: 0.5659 - val_loss: 1.1054 - val_acc: 0.5684\n",
            "Epoch 12/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0922 - acc: 0.5694 - val_loss: 1.1013 - val_acc: 0.5665\n",
            "Epoch 13/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0868 - acc: 0.5712 - val_loss: 1.0978 - val_acc: 0.5707\n",
            "Epoch 14/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0828 - acc: 0.5729 - val_loss: 1.0964 - val_acc: 0.5717\n",
            "Epoch 15/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0801 - acc: 0.5746 - val_loss: 1.0970 - val_acc: 0.5682\n",
            "Epoch 16/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0780 - acc: 0.5748 - val_loss: 1.0944 - val_acc: 0.5693\n",
            "Epoch 17/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0764 - acc: 0.5747 - val_loss: 1.0938 - val_acc: 0.5699\n",
            "Epoch 18/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0752 - acc: 0.5764 - val_loss: 1.0942 - val_acc: 0.5685\n",
            "Epoch 19/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0742 - acc: 0.5756 - val_loss: 1.0923 - val_acc: 0.5713\n",
            "Epoch 20/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0739 - acc: 0.5762 - val_loss: 1.0918 - val_acc: 0.5731\n",
            "Epoch 21/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0730 - acc: 0.5758 - val_loss: 1.0939 - val_acc: 0.5735\n",
            "Epoch 22/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0723 - acc: 0.5771 - val_loss: 1.0919 - val_acc: 0.5728\n",
            "Epoch 23/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0720 - acc: 0.5773 - val_loss: 1.0925 - val_acc: 0.5744\n",
            "Epoch 24/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0716 - acc: 0.5766 - val_loss: 1.0919 - val_acc: 0.5729\n",
            "Epoch 25/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0713 - acc: 0.5770 - val_loss: 1.0915 - val_acc: 0.5711\n",
            "Epoch 26/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0710 - acc: 0.5777 - val_loss: 1.0914 - val_acc: 0.5754\n",
            "Epoch 27/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0707 - acc: 0.5776 - val_loss: 1.0912 - val_acc: 0.5723\n",
            "Epoch 28/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0706 - acc: 0.5772 - val_loss: 1.0928 - val_acc: 0.5714\n",
            "Epoch 29/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0702 - acc: 0.5777 - val_loss: 1.0905 - val_acc: 0.5716\n",
            "Epoch 30/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0702 - acc: 0.5771 - val_loss: 1.0905 - val_acc: 0.5746\n",
            "Epoch 31/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0701 - acc: 0.5774 - val_loss: 1.0905 - val_acc: 0.5721\n",
            "Epoch 32/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0700 - acc: 0.5773 - val_loss: 1.0979 - val_acc: 0.5755\n",
            "Epoch 33/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0699 - acc: 0.5776 - val_loss: 1.0905 - val_acc: 0.5727\n",
            "Epoch 34/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0696 - acc: 0.5772 - val_loss: 1.0912 - val_acc: 0.5739\n",
            "Epoch 35/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0696 - acc: 0.5778 - val_loss: 1.0905 - val_acc: 0.5736\n",
            "Epoch 36/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0694 - acc: 0.5773 - val_loss: 1.0919 - val_acc: 0.5709\n",
            "Epoch 37/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0694 - acc: 0.5772 - val_loss: 1.0925 - val_acc: 0.5746\n",
            "Epoch 38/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0695 - acc: 0.5771 - val_loss: 1.0903 - val_acc: 0.5731\n",
            "Epoch 39/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0695 - acc: 0.5773 - val_loss: 1.0924 - val_acc: 0.5753\n",
            "Epoch 40/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0692 - acc: 0.5779 - val_loss: 1.0944 - val_acc: 0.5757\n",
            "Epoch 41/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0691 - acc: 0.5775 - val_loss: 1.0906 - val_acc: 0.5714\n",
            "Epoch 42/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0692 - acc: 0.5773 - val_loss: 1.0902 - val_acc: 0.5738\n",
            "Epoch 43/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0690 - acc: 0.5772 - val_loss: 1.0934 - val_acc: 0.5694\n",
            "Epoch 44/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0689 - acc: 0.5777 - val_loss: 1.0908 - val_acc: 0.5731\n",
            "Epoch 45/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0688 - acc: 0.5773 - val_loss: 1.0900 - val_acc: 0.5739\n",
            "Epoch 46/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0689 - acc: 0.5782 - val_loss: 1.0909 - val_acc: 0.5716\n",
            "Epoch 47/50\n",
            "109242/109242 [==============================] - 18s 169us/step - loss: 1.0689 - acc: 0.5780 - val_loss: 1.0916 - val_acc: 0.5727\n",
            "Epoch 48/50\n",
            "109242/109242 [==============================] - 18s 168us/step - loss: 1.0687 - acc: 0.5776 - val_loss: 1.0906 - val_acc: 0.5743\n",
            "Epoch 49/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0687 - acc: 0.5775 - val_loss: 1.0904 - val_acc: 0.5743\n",
            "Epoch 50/50\n",
            "109242/109242 [==============================] - 18s 167us/step - loss: 1.0687 - acc: 0.5782 - val_loss: 1.0901 - val_acc: 0.5736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb8b80fd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgqUH1__1E1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "199c3025-5dff-4a00-cda5-cb5f634a149c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "y_pred = model.predict_classes(x_test, batch_size=128, verbose=0)\n",
        "y_pred1=np.argmax(y_test_np, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_pred1, y_pred)\n",
        "precision = precision_score(y_pred1, y_pred, average='weighted')\n",
        "f1 = f1_score(y_pred1, y_pred, average='weighted')\n",
        "recall = recall_score(y_pred1, y_pred, average='weighted')\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('Precision: %f' % precision)\n",
        "print('Recall: %f' % recall)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.573583\n",
            "Precision: 0.535278\n",
            "Recall: 0.573583\n",
            "F1 score: 0.522006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNOxJDup3eFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('0888759_1dConv_reg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHhd78FY1NKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('0888759_1dConv_reg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MlEBIKt1lSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IynFjmSr53wJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sNL0pDP6P5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nChc7He6ZiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcNKCUDJf3dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_oJsOgQuQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCCUt618Q32l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ov2TXoxRDLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9XSiBD-RIs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}